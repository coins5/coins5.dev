---
title: "Structured Logging with Loguru: JSON for Observability Tools"
author: Marlon Colca
description: "When you're sending logs to ELK, Datadog, or any log aggregator, plain text isn't enough. "
image:
  url: "https://docs.astro.build/assets/rays.webp"
  alt: "The Astro logo on a dark background with rainbow rays."
pubDate: 2025-05-03
tags: ["python", "logging", "loguru"]
---

When you're sending logs to ELK, Datadog, or any log aggregator, plain text isn't enough.  
You need **structured logs** ‚Äî usually in **JSON** ‚Äî so systems can index and search them properly.

In this post, you'll learn how to output JSON logs using Loguru and make them production-ready.

---

## üîß Why Structured Logging?

Structured logs make it easy to:

- Search logs by fields (e.g. user_id, request_id).
- Visualize errors by service or severity.
- Connect logs to metrics and traces.

Here‚Äôs a classic unstructured log:

```text
2025-05-08 10:22:43 | INFO | User created successfully: Marlon
```

Now in JSON:

```json
{
  "time": "2025-05-08T10:22:43",
  "level": "INFO",
  "message": "User created successfully",
  "user": "Marlon"
}
```

Much more powerful.

## üõ†Ô∏è Custom JSON Formatter in Loguru

You can define your own log format using a function:

```python
import json
from loguru import logger
from datetime import datetime

def json_formatter(record):
    log = {
        "time": record["time"].strftime("%Y-%m-%dT%H:%M:%S"),
        "level": record["level"].name,
        "message": record["message"],
        "module": record["module"],
        "function": record["function"],
        "line": record["line"],
    }
    return json.dumps(log) + "\n"

logger.remove()
logger.add("logs.json", format=json_formatter)

```

This will output structured logs that are ready for ingestion.

## üîó Add Custom Fields

You can pass extra fields with `extra={}`:

```python
logger.bind(user="marlon", action="create").info("User created")

```

And update the formatter:

```python
def json_formatter(record):
    log = {
        "time": record["time"].strftime("%Y-%m-%dT%H:%M:%S"),
        "level": record["level"].name,
        "message": record["message"],
        "module": record["module"],
        "function": record["function"],
        "line": record["line"],
        **record["extra"]
    }
    return json.dumps(log) + "\n"
```

Output:

```json
{
  "time": "2025-05-08T10:22:43",
  "level": "INFO",
  "message": "User created",
  "user": "marlon",
  "action": "create"
}
```

## üöÄ Stream to Console + File in JSON

```python
logger.add(sys.stdout, format=json_formatter)
logger.add("structured.json", format=json_formatter)

```

## üß™ Example: FastAPI with Structured Logs

```python
@app.get("/")
async def hello():
    logger.bind(request_id="abc123", user_id=42).info("Hello endpoint hit")
    return {"message": "hi"}

```

## ‚úÖ Final Thoughts

With just a few lines, you‚Äôve turned your logs into machine-readable, structured data ‚Äî ready for search, filtering, and dashboards.

- Use this setup to integrate seamlessly with:
- ELK / OpenSearch
- Datadog
- Grafana Loki
- Any system that speaks JSON

## üîú Coming up next

üëâ _Async Logging with Loguru: Handling concurrency without losing logs_
